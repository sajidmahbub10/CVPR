{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/AdityaLoth/Mask-Detection-using-Google-Colab/blob/master/mask_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"8gSinmvLiaJ4"},"source":["# **FACE MASK DETECTION**\n","\n","During this pandemic people are coming with various new and creative ideas to tackle Covid-19. One such idea is to detect whether a person is wearing mask or not at public places or at their workplaces. This has been made possible with the help of Deep Learning and Computer Vision.\n","This notebook took inspiration from one such [PROJECT](https://github.com/chandrikadeb7/Face-Mask-Detection) by Chandrika Deb.This is a Google colab implementation of Face Mask detection project. \n","\n","\n","This notebook is divided into 4 parts:\n","\n","1.   Setting up the environment\n","2.   Training the model\n","3.   Detection on Images\n","4.   Detection on Videos(using WebCam)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VNsWU7VGo9s2"},"source":["\n","\n","---\n","# PART 1: Setting up the environment\n"]},{"cell_type":"markdown","metadata":{"id":"1tGhPsvBpmrQ"},"source":["**STEP 1. Connect the Colab notebook to Google Drive**\n","\n","We're gonna map your Google Drive folder. This first step is the only one that will require your manual interaction every time you run your notebook.\n","\n","* Execute the following cell _(Click on Play button or press CTRL + ENTER)_ and click on the link to authorize your notebook to access to your Google Drive. \n","* Paste the code Google will give to you and push `enter`"]},{"cell_type":"code","metadata":{"id":"PlyJlElKs4tW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682879362608,"user_tz":-360,"elapsed":28710,"user":{"displayName":"sajid mahbub","userId":"10482410205823991067"}},"outputId":"b20c9ae7-2d56-4667-bcaf-51bf4b8ee224"},"source":["# This cell imports the drive library and mounts your Google Drive as a VM local drive. You can access to your Drive files \n","# using this path \"/content/drive/My Drive/\"\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"V3xT9RU3tg_d"},"source":["**STEP 2. Creating a new directory named MASKDETECTION and changing to that directory**\n","\n","Run the following cells to create a folder named MASKDETECTION on your drive.\n","\n","We then change the directory to this folder.\n","\n","**NOTE:** You need to create the directory only once. So skip creating directory on further runs."]},{"cell_type":"code","metadata":{"id":"zOqxY4eLvWWB","executionInfo":{"status":"ok","timestamp":1682879368365,"user_tz":-360,"elapsed":1393,"user":{"displayName":"sajid mahbub","userId":"10482410205823991067"}}},"source":["# This creates a new directory in your drive\n","\n","%mkdir drive/My\\ Drive/MASKDETECTION"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"3OqXnABIKohZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682879374521,"user_tz":-360,"elapsed":2,"user":{"displayName":"sajid mahbub","userId":"10482410205823991067"}},"outputId":"c4a7e28e-2d54-44ef-a971-4407f3c934cb"},"source":["#This makes the newly created directory your working directory\n","\n","%cd drive/My\\ Drive/MASKDETECTION"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/MASKDETECTION\n"]}]},{"cell_type":"markdown","metadata":{"id":"i06osx-Lw3cW"},"source":["**STEP 3. Downloading dataset and required files**\n","\n","Now we download the image dataset of with and without mask images into our folder. We also need weights and configuration file.\n","\n","**NOTE:** Remember that you need to successfully download these files only once. You can skip this step on futher runs."]},{"cell_type":"code","metadata":{"id":"jB31VWwq3DTR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682879381322,"user_tz":-360,"elapsed":8,"user":{"displayName":"sajid mahbub","userId":"10482410205823991067"}},"outputId":"d91eb060-14aa-4071-a27f-49485e7edf5c"},"source":["!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1P9YgcPTZNufjC45YhtIJoGMz5Y4KtJfo' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1P9YgcPTZNufjC45YhtIJoGMz5Y4KtJfo\" -O FILE && rm -rf /tmp/cookies.txt"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-04-30 18:29:38--  https://docs.google.com/uc?export=download&confirm=&id=1P9YgcPTZNufjC45YhtIJoGMz5Y4KtJfo\n","Resolving docs.google.com (docs.google.com)... 142.251.2.101, 142.251.2.102, 142.251.2.138, ...\n","Connecting to docs.google.com (docs.google.com)|142.251.2.101|:443... connected.\n","HTTP request sent, awaiting response... 404 Not Found\n","2023-04-30 18:29:38 ERROR 404: Not Found.\n","\n"]}]},{"cell_type":"code","metadata":{"id":"pJoAi336AZd2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682879384988,"user_tz":-360,"elapsed":6,"user":{"displayName":"sajid mahbub","userId":"10482410205823991067"}},"outputId":"0fbca779-2a43-4b21-8796-d32bf8d4c1b6"},"source":["!unzip FILE"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  FILE\n","  End-of-central-directory signature not found.  Either this file is not\n","  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n","  latter case the central directory and zipfile comment will be found on\n","  the last disk(s) of this archive.\n","unzip:  cannot find zipfile directory in one of FILE or\n","        FILE.zip, and cannot find FILE.ZIP, period.\n"]}]},{"cell_type":"markdown","metadata":{"id":"wvcUGa3rA9i2"},"source":["**STEP 4. Importing required modules**"]},{"cell_type":"code","metadata":{"id":"6rl4D9CEzdvh","executionInfo":{"status":"ok","timestamp":1682879395167,"user_tz":-360,"elapsed":5657,"user":{"displayName":"sajid mahbub","userId":"10482410205823991067"}}},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.layers import AveragePooling2D\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.preprocessing.image import load_img\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import imutils\n","from imutils import paths\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import argparse\n","import os\n","import cv2\n","from google.colab.patches import cv2_imshow\n","from imutils.video import VideoStream\n","import time\n","import sys"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J9RCNIxJwfwo"},"source":["**Step 5. Importing the face detection model**\n","\n","Since our main focus here is to detect masks we will not go into creating a face detection model. We simply import a pretrained model res10_300x300_ssd_iter_140000.caffemodel."]},{"cell_type":"code","metadata":{"id":"-ZItzaMLpCc-","colab":{"base_uri":"https://localhost:8080/","height":217},"executionInfo":{"status":"error","timestamp":1682879440176,"user_tz":-360,"elapsed":499,"user":{"displayName":"sajid mahbub","userId":"10482410205823991067"}},"outputId":"280fae21-6437-41f0-d5de-438322e4b0fd"},"source":["#Setting path to configuration file\n","prototxtPath = os.path.sep.join([\"face_detector\", \"deploy.prototxt\"])\n","\n","#setting path to weights\n","weightsPath = os.path.sep.join([\"face_detector\",\n","\t\"res10_300x300_ssd_iter_140000.caffemodel\"])\n","\n","#Creating the network to detect faces\n","net = cv2.dnn.readNet(prototxtPath, weightsPath)"],"execution_count":8,"outputs":[{"output_type":"error","ename":"error","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-6b81910a2514>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#Creating the network to detect faces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprototxtPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweightsPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /io/opencv/modules/dnn/src/caffe/caffe_io.cpp:1126: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"face_detector/deploy.prototxt\" in function 'ReadProtoFromTextFile'\n"]}]},{"cell_type":"markdown","metadata":{"id":"ep1qSlsHEqIY"},"source":["\n","\n","\n","\n","\n","\n","---\n","\n","\n","\n","\n","\n","# PART 2: Training the model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DfSP0XOxXCoJ"},"source":["**Step 1. Converting the images to array and storing their labels(with mask or withour mask)**"]},{"cell_type":"code","metadata":{"id":"zfFk99I6MrIF","colab":{"base_uri":"https://localhost:8080/","height":74},"outputId":"04f14379-3e0f-481e-965e-08faf885716f"},"source":["imagePaths = list(paths.list_images(\"dataset\"))\n","data = []\n","labels = []\n","\n","# loop over the image paths\n","for imagePath in imagePaths:\n","\t# extract the class label from the filename\n","\tlabel = imagePath.split(os.path.sep)[-2]\n","\n","\t# load the input image (224x224) and preprocess it\n","\timage = load_img(imagePath, target_size=(224, 224))\n","\timage = img_to_array(image)\n","\timage = preprocess_input(image)\n","\n","\t# update the data and labels lists, respectively\n","\tdata.append(image)\n","\tlabels.append(label)\n"," \n","data = np.array(data, dtype=\"float32\")\n","labels = np.array(labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"LLjsItmfPKsx"},"source":["# perform one-hot encoding on the labels\n","lb = LabelBinarizer()\n","labels = lb.fit_transform(labels)\n","labels = to_categorical(labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wocmrbz6ZjIP"},"source":["**Step 2. Dividing the dataset for training and testing purposes**"]},{"cell_type":"code","metadata":{"id":"xj3pHKFUVxfo"},"source":["(trainX, testX, trainY, testY) = train_test_split(data, labels,\n","\ttest_size=0.20, stratify=labels, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q7rji4J6Z0vz"},"source":["**Step 3. Creating the model**\n","\n","We will use MobileNetv2 as our base model and then apply further layers over it to create a final model."]},{"cell_type":"code","metadata":{"id":"LCmnVgzLaCf5","colab":{"base_uri":"https://localhost:8080/","height":93},"outputId":"87f610e5-7e8b-4cd2-bfd2-653e3e571233"},"source":["baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n","\tinput_tensor=Input(shape=(224, 224, 3)))\n","\n","# construct the head of the model that will be placed on top of the\n","# the base model\n","headModel = baseModel.output\n","headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n","headModel = Flatten(name=\"flatten\")(headModel)\n","headModel = Dense(128, activation=\"relu\")(headModel)\n","headModel = Dropout(0.5)(headModel)\n","headModel = Dense(2, activation=\"softmax\")(headModel)\n","\n","# place the head FC model on top of the base model (this will become\n","# the actual model we will train)\n","model = Model(inputs=baseModel.input, outputs=headModel)\n","\n","# loop over all layers in the base model and freeze them so they will\n","# *not* be updated during the first training process\n","for layer in baseModel.layers:\n","\tlayer.trainable = False"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n","9412608/9406464 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"n18D8AfBaYLq"},"source":["Compiling our model"]},{"cell_type":"code","metadata":{"id":"oRxQnai4YHXd"},"source":["LR = 1e-4     #learning rate\n","EPOCHS = 20    #no. of epochs\n","BS = 32       #batch size\n","\n","#We will use Adam Optimizer for our model\n","opt= Adam(lr=LR, decay=LR / EPOCHS)\n","\n","#To compile the model we use 'binary crossentropy' as loss function and set the metrics to accuracy\n","model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n","metrics=[\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sKO9yvoHeCYE"},"source":["Construct the training image generator for data augmentation\n"]},{"cell_type":"code","metadata":{"id":"RCb88i0-ew0q"},"source":["aug = ImageDataGenerator(\n","\trotation_range=20,\n","\tzoom_range=0.15,\n","\twidth_shift_range=0.2,\n","\theight_shift_range=0.2,\n","\tshear_range=0.15,\n","\thorizontal_flip=True,\n","\tfill_mode=\"nearest\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rziv9qM5fmfa"},"source":["Training our model"]},{"cell_type":"code","metadata":{"id":"J8N4uMW6Ye-P"},"source":["H = model.fit(\n","\taug.flow(trainX, trainY, batch_size=BS),\n","\tsteps_per_epoch=len(trainX) // BS,\n","\tvalidation_data=(testX, testY),\n","\tvalidation_steps=len(testX) // BS,\n","\tepochs=EPOCHS)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ULcA2MVXfxPk"},"source":["Make predictions on the testing set"]},{"cell_type":"code","metadata":{"id":"P0U5phmkowwM"},"source":["preds = model.evaluate(testX, testY)\n","print (\"Loss = \" + str(preds[0]))\n","print (\"Test Accuracy = \" + str(preds[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5evDd92lf8hs"},"source":["Serialize the model to disk"]},{"cell_type":"code","metadata":{"id":"iTlD5_aIxZhs"},"source":["#Here \"model\" is the name of model. You can change it accordingly.\n","\n","model.save(\"model\", save_format=\"h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5b7hRAAtgJE8"},"source":["\n","\n","\n","---\n","\n","\n","# PART 3. Detection on Images"]},{"cell_type":"markdown","metadata":{"id":"SYPJy0HpgvGT"},"source":["Let us load the model which we created."]},{"cell_type":"code","metadata":{"id":"xhte2lIRxrho"},"source":["#This will load the pretrained model named 'mask_detector'. You can replace it with your model name.\n","\n","model = load_model(\"mask_detector.model\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DGG-WILjiHnB"},"source":["Importing the image for mask detection"]},{"cell_type":"code","metadata":{"id":"Ao3vBleYyN1e","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":93},"outputId":"29ebb185-6f3f-45f9-e383-6c7b398b234c"},"source":["from google.colab import files\n","uploaded = files.upload() \n","for name, data in uploaded.items():\n","  with open(name, 'wb') as f:\n","    f.write(data)\n","    print ('saved file', name)\n","    image = cv2.imread(name)\n","    orig = image.copy()\n","    (h, w) = image.shape[:2]\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-b60c00a2-4a57-4fe0-8eca-db14913f1fd2\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-b60c00a2-4a57-4fe0-8eca-db14913f1fd2\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving smile.jpg to smile.jpg\n","saved file smile.jpg\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MYD1I9WLibPT"},"source":["Construct a blob from the image and then pass it through the imported face detector network to obtain face detections"]},{"cell_type":"code","metadata":{"id":"ne400mbLzLdK"},"source":["blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300),\n","\t(104.0, 177.0, 123.0))\n","\n","net.setInput(blob)\n","detections = net.forward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y641TS8mjQSO"},"source":["Looping over the detections and labelling them as 'Mask' or 'No mask'"]},{"cell_type":"code","metadata":{"id":"n8Wmo4T20RJb"},"source":["for i in range(0, detections.shape[2]):\n","\t# extract the confidence (i.e., probability) associated with\n","\t# the detection\n","\tconfidence = detections[0, 0, i, 2]\n","\n","\t# filter. out weak detections by ensuring the confidence is\n","\t# greater than the minimum confidence\n","\tif confidence > 0.5:\n","\t\t# compute the (x, y)-coordinates of the bounding box for\n","\t\t# the object\n","\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n","\n","\t\t# ensure the bounding boxes fall within the dimensions of\n","\t\t# the frame\n","\t\t(startX, startY) = (max(0, startX), max(0, startY))\n","\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n","\n","\t\t# extract the face ROI, convert it from BGR to RGB channel\n","\t\t# ordering, resize it to 224x224, and preprocess it\n","\t\tface = image[startY:endY, startX:endX]\n","\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n","\t\tface = cv2.resize(face, (224, 224))\n","\t\tface = img_to_array(face)\n","\t\tface = preprocess_input(face)\n","\t\tface = np.expand_dims(face, axis=0)\n","\n","\t\t# pass the face through the model to determine if the face\n","\t\t# has a mask or not\n","\t\t(mask, withoutMask) = model.predict(face)[0]\n","\n","\t\t# determine the class label and color we'll use to draw\n","\t\t# the bounding box and text\n","\t\tlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\n","\t\tcolor = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n","\n","\t\t# include the probability in the label\n","\t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n","\n","\t\t# display the label and bounding box rectangle on the output\n","\t\t# frame\n","\t\tcv2.putText(image, label, (startX, startY - 10),\n","\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n","\t\tcv2.rectangle(image, (startX, startY), (endX, endY), color, 2)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PA8ZA5sjllXg"},"source":["Displaying the final image"]},{"cell_type":"code","metadata":{"id":"2xnho_vz0waB"},"source":["cv2_imshow(image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7rFgqWH9mHS6"},"source":["\n","\n","---\n","\n","# Part 4. Detection on videos (using WebCam)\n"]},{"cell_type":"markdown","metadata":{"id":"mlKkTYjxm3Xr"},"source":["A major challange which I faced was the real time detection on objects(masks).\n","Colab codes executes on a VM that doesn't have a webcam attached. So real time object detection is not possible here. But with the help of Javascript we can use the webcam to capture images and videos and use them for further actions. Though it is not real time but it works."]},{"cell_type":"markdown","metadata":{"id":"sDO7KmVSrTRk"},"source":["First of all we need to set the webcam with the following code. Your camera will start if it is working.\n","Clear the output afterwards to stop streaming."]},{"cell_type":"code","metadata":{"id":"j_VGKBWe7iBI","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"1ccac090-8d47-4478-bb7e-1de917b3b73c"},"source":["!pip install ffmpeg-python\n","\n","\n","from IPython.display import HTML, Javascript, display\n","from google.colab.output import eval_js\n","from base64 import b64decode\n","import numpy as np\n","import io\n","import ffmpeg\n","\n","video_file= '/content/drive/My Drive/MASKDETECTION/train.mp4' \n","\n","VIDEO_HTML = \"\"\"\n","<script>\n","var my_div = document.createElement(\"DIV\");\n","var my_p = document.createElement(\"P\");\n","var my_btn = document.createElement(\"BUTTON\");\n","var my_btn_txt = document.createTextNode(\"Press to start recording\");\n","\n","my_btn.appendChild(my_btn_txt);\n","my_div.appendChild(my_btn);\n","document.body.appendChild(my_div);\n","\n","var base64data = 0;\n","var reader;\n","var recorder, videoStream;\n","var recordButton = my_btn;\n","\n","var handleSuccess = function(stream) {\n","  videoStream = stream;\n","  var options = {  \n","    mimeType : 'video/webm;codecs=vp9'  \n","  };            \n","  recorder = new MediaRecorder(stream, options);\n","  recorder.ondataavailable = function(e) {            \n","    var url = URL.createObjectURL(e.data);\n","    var preview = document.createElement('video');\n","    preview.controls = true;\n","    preview.src = url;\n","    document.body.appendChild(preview);\n","\n","    reader = new FileReader();\n","    reader.readAsDataURL(e.data); \n","    reader.onloadend = function() {\n","      base64data = reader.result;\n","    }\n","  };\n","  recorder.start();\n","  };\n","\n","recordButton.innerText = \"Recording... press to stop\";\n","\n","navigator.mediaDevices.getUserMedia({video: true}).then(handleSuccess);\n","\n","\n","function toggleRecording() {\n","  if (recorder && recorder.state == \"recording\") {\n","      recorder.stop();\n","      videoStream.getVideoTracks()[0].stop();\n","      recordButton.innerText = \"Original recording\"\n","  }\n","}\n","\n","function sleep(ms) {\n","  return new Promise(resolve => setTimeout(resolve, ms));\n","}\n","\n","var data = new Promise(resolve=>{\n","recordButton.onclick = ()=>{\n","toggleRecording()\n","\n","sleep(2000).then(() => {\n","  // wait 2000ms for the data to be available\n","  resolve(base64data.toString())\n","\n","});\n","\n","}\n","});\n","      \n","</script>\n","\"\"\"\n","\n","def start_webcam():\n","  js = Javascript('''\n","    async function startWebcam() {\n","      const div = document.createElement('div');\n","\n","      const video = document.createElement('video');\n","      video.style.display = 'block';\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","      \n","      return;\n","\n","    }\n","    ''')\n","  \n","  display(js)\n","  data = eval_js('startWebcam()')\n","  \n","start_webcam()\n","\n","def get_video():\n","  display(HTML(VIDEO_HTML))\n","  data = eval_js(\"data\")\n","  binary = b64decode(data.split(',')[1])\n","  \n","  return binary\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.6/dist-packages (0.2.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from ffmpeg-python) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"b83Q4ZEtsKti"},"source":["Creating a function to detect and predict masks "]},{"cell_type":"code","metadata":{"id":"nh3ljMsm9MkP"},"source":["def detect_and_predict_mask(frame, faceNet, maskNet):\n","\t# grab the dimensions of the frame and then construct a blob\n","\t# from it\n","\t(h, w) = frame.shape[:2]\n","\tblob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n","\t\t(104.0, 177.0, 123.0))\n","\n","\t# pass the blob through the network and obtain the face detections\n","\tfaceNet.setInput(blob)\n","\tdetections = faceNet.forward()\n","\n","\t# initialize our list of faces, their corresponding locations,\n","\t# and the list of predictions from our face mask network\n","\tfaces = []\n","\tlocs = []\n","\tpreds = []\n","\n","\t# loop over the detections\n","\tfor i in range(0, detections.shape[2]):\n","\t\t# extract the confidence (i.e., probability) associated with\n","\t\t# the detection\n","\t\tconfidence = detections[0, 0, i, 2]\n","\n","\t\t# filter out weak detections by ensuring the confidence is\n","\t\t# greater than the minimum confidence\n","\t\tif confidence > 0.5:\n","\t\t\t# compute the (x, y)-coordinates of the bounding box for\n","\t\t\t# the object\n","\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n","\n","\t\t\t# ensure the bounding boxes fall within the dimensions of\n","\t\t\t# the frame\n","\t\t\t(startX, startY) = (max(0, startX), max(0, startY))\n","\t\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n","\n","\t\t\t# extract the face ROI, convert it from BGR to RGB channel\n","\t\t\t# ordering, resize it to 224x224, and preprocess it\n","\t\t\tface = frame[startY:endY, startX:endX]\n","\t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n","\t\t\tface = cv2.resize(face, (224, 224))\n","\t\t\tface = img_to_array(face)\n","\t\t\tface = preprocess_input(face)\n","\n","\t\t\t# add the face and bounding boxes to their respective\n","\t\t\t# lists\n","\t\t\tfaces.append(face)\n","\t\t\tlocs.append((startX, startY, endX, endY))\n","\n","\t# only make a predictions if at least one face was detected\n","\tif len(faces) > 0:\n","\t\t# for faster inference we'll make batch predictions on *all*\n","\t\t# faces at the same time rather than one-by-one predictions\n","\t\t# in the above `for` loop\n","\t\tfaces = np.array(faces, dtype=\"float32\")\n","\t\tpreds = maskNet.predict(faces, batch_size=32)\n","\n","\t# return a 2-tuple of the face locations and their corresponding\n","\t# locations\n","\treturn (locs, preds)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TW38X-xCsVP7"},"source":["Load the model which we created."]},{"cell_type":"code","metadata":{"id":"FYBVZjzOAFeP"},"source":["maskNet = load_model(\"mask_detector.model\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xAQ-mig-tgWZ"},"source":["Capturing the video and detecting faces with or without mask"]},{"cell_type":"code","metadata":{"id":"WXkLsgx4Awh7"},"source":["vid = get_video()\n","\n","with open(video_file, 'wb') as f:\n","  f.write(vid)\n","print(\"Recorded the video\")\n","\n","\n","video = cv2.VideoCapture('train.mp4')\n","out = cv2.VideoWriter('out.avi', cv2.VideoWriter_fourcc(*'XVID'), 20.0, (640,480))\n","# print(type(out))\n","\n","\n","while True:\n","\t\t\t# grab the frame from the threaded video stream and resize it\n","\t\t\t# to have a maximum width of 400 pixels\n","\t\t\t# frame = vs.read()\n","\t\t\t# frame = imutils.resize(frame, width=400)\n","\t\tboool, frame = video.read()\n","\t\t\n","\t\tif(boool==False):\n","\t\t\tbreak\n","\t\tw, h, c = frame.shape\n","\n","\t\t#syntax: cv2.resize(img, (width, height))\n","\t\tframe = cv2.resize(frame,(h, w))\n","\n","\t\t# detect faces in the frame and determine if they are wearing a\n","\t\t# face mask or not\n","\t\tlocs, preds = detect_and_predict_mask(frame, net, maskNet)\n","\t\t# loop over the detected face locations and their corresponding\n","\t\t# locations\n","\n","\t \n","\t\tfor (box, pred) in zip(locs, preds):\n","\t\t\t\t\t# unpack \t# frame = vs.read()\n","\t\t\t\t\t# frame = imutils.resize(frame, width=400)the bounding box and predictions\n","\t\t\t\t\t(startX, startY, endX, endY) = box\n","\t\t\t\t\t(mask, withoutMask) = pred\n","\n","\t\t\t\t\t# determine the class label and color we'll use to draw\n","\t\t\t\t\t# the bounding box and text\n","\t\t\t\t\tlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\n","\t\t\t\t\tcolor = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n","\n","\t\t\t\t\t# include the probability in the label\n","\t\t\t\t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n","\n","\t\t\t\t\t# display the label and bounding box rectangle on the output\n","\t\t\t\t\t# frame\n","\t\t\t\t\tcv2.putText(frame, label, (startX, startY - 10),\n","\t\t\t\t\tcv2.FONT_HERSHEY_COMPLEX, 0.55, color, 2)\n","\t\t\t\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n","\n","\t\t# show the output frame\n","\t\tout.write(frame)\n","\t\t# cv2_imshow(frame)\n","\n","out.release()\n","# do a bit of cleanup\n","cv2.destroyAllWindows()\n","\n","print(\"Detection complete.Now converting\")\n","\n","!ffmpeg -i out.avi output.mp4 -loglevel quiet -y \n","\n","from IPython.display import HTML\n","from base64 import b64encode\n","mp4 = open('output.mp4','rb').read()\n","data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","\n","HTML(\"\"\"\n","<video controls>\n","      <source src=\"%s\" type=\"video/mp4\">\n","</video>\n","\"\"\" % data_url)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-glkNJBivaZ8"},"source":["\n","\n","---\n","\n","# SOURCES\n","\n","\n","1.   chandrikadeb7/Face-Mask-Detection repo [github](https://github.com/chandrikadeb7/Face-Mask-Detection)\n","2.   [Tensorflow](https://www.tensorflow.org)\n","\n","\n","\n","\n","\n","\n","\n","## **THANK YOU!**"]}]}